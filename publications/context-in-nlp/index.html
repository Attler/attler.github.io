<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <title>Context in nlp</title>
  <meta property="og:title" content="Context in nlp" />
  <meta property="og:image" content="http://attler.github.io/img/me.png" />
  <meta name="description" content="Computer Science Researcher">
  <meta property="og:description" content="Computer Science Researcher" />
  <meta name="author" content="Jason Hepburn">
  
  <link href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/4.0.0/css/bootstrap.min.css" rel="stylesheet">
  
  <link href="https://fonts.googleapis.com/css?family=Saira+Extra+Condensed:100,200,300,400,500,600,700,800,900" rel="stylesheet">
  <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,300i,400,400i,600,600i,700,700i,800,800i" rel="stylesheet">
  <link href="https://use.fontawesome.com/releases/v5.5.0/css/all.css" rel="stylesheet">
  <link href='https://cdnjs.cloudflare.com/ajax/libs/devicons/1.8.0/css/devicons.min.css' rel='stylesheet'>
  <link href="https://cdnjs.cloudflare.com/ajax/libs/simple-line-icons/2.4.1/css/simple-line-icons.min.css" rel="stylesheet">
  
  <link href="http://attler.github.io/css/resume.css" rel="stylesheet">
  <link href="http://attler.github.io/css/tweaks.css" rel="stylesheet">
  <meta name="generator" content="Hugo 0.56.0" />
  
   
  
</head>
<body id="page-top">
  <nav class="navbar navbar-expand-lg navbar-dark bg-primary fixed-top" id="sideNav">
  <a class="navbar-brand js-scroll-trigger" href="#page-top">
    <span class="d-block d-lg-none">Jason Hepburn</span>
    <span class="d-none d-lg-block">
      <img class="img-fluid img-profile rounded-circle mx-auto mb-2" src="https://pbs.twimg.com/profile_images/1086252896813932544/uGZYMwtZ_400x400.jpg" alt="">
    </span>
  </a>
  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>
  <div class="collapse navbar-collapse" id="navbarSupportedContent">
    <ul class="navbar-nav">
      <li class="nav-item">
        <a class="nav-link js-scroll-trigger" href="/#about">About</a>
      </li>
      
          <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="/#skills">Skills</a>
          </li>
      
      
          <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="/#projects">Projects</a>
          </li>
      
      
      
          <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="/#publications">Publications</a>
          </li>
      
      
          <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="/#experience">Experience</a>
          </li>
      
      
          <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="/#education">Education</a>
          </li>
      
      
    </ul>
  </div>
</nav>

  <div class="container-fluid p-0">
    
<nav aria-label="breadcrumb">
  <ol  class="breadcrumb">
    





<li class="breadcrumb-item">
  <a href="http://attler.github.io/">JJ</a>
</li>


<li class="breadcrumb-item">
  <a href="http://attler.github.io/publications/">Publications</a>
</li>


<li class="breadcrumb-item active">
  <a href="http://attler.github.io/publications/context-in-nlp/">Context in nlp</a>
</li>

  </ol>
</nav>




<section class="resume-section p-3 p-lg-5 d-flex d-column content">
  <div class="my-auto">
    <h2 class="mb-0"><span class="text-primary">Context in nlp</span></h2>
    
    
    

<p>One of the most important problems in Natural Langugage Understanding (NLU) is context. Not all words are different to each other in the same way, which is why models based on word frequency can perform poorly. Word embeddings such as <a href="http://www.aclweb.org/anthology/D14-1162">GLOVE</a> are often used in order to represent the similarities and differences words or tokens. Words that are similar in meaning are represented closer together in the embedding space. Some words though have very different meanings in different contexts. For example, &ldquo;bank&rdquo; may mean the &ldquo;bank&rdquo; of a river (noun), a savings &ldquo;bank&rdquo; (noun), to &ldquo;bank&rdquo; money (verb), or a &ldquo;bank&rdquo; of computers (noun). This is difficult to represent and resolve the ambiguity with embeddings.</p>

<h2 id="cnn">CNN</h2>

<p>The context of a word is not just determined by what words are around it but also their order and the syntax of the string. In NLU tasks such as text classification input documents are often encoded in to some representation of the whole document. This can be done using Convolutional Neural Networks <a href="http://arxiv.org/abs/1408.5882">(CNN)</a> which allows the context of otherwise ambiguous words to be represented in the encoding.</p>

<h2 id="lstm">LSTM</h2>

<p><a href="http://arxiv.org/abs/1409.3215">Sequence-to-sequence</a> models are often used for abstractive text summarisation tasks. The architecture is made up of an encoder which produces a representation of the input sequence and a decoder that produces an output sequence using this encoding as context. Recurrent Neural Networks (RNN) such as LSTM&rsquo;s are used for the encoders and decoders. This means the decoders input context is fixed at each step of the output generation.</p>

<h2 id="attention">Attention</h2>

<p><a href="https://arxiv.org/abs/1409.0473">Attention</a> is a mechanism for generating a unique context vector for each step of the decoder sequence. This means that the encoder does not need to encode all information from the input into a single fixed vector. The context vector for each step of the decoder is a weighted sum of the hidden states of the input sequence. These weights are calculated with a <em>score</em> function taking the hidden states of the encoder and decoder as input. This score represents the how much attention should be given to each token of the input sequence at the current step of the output sequence. Attention allows for a dynamic context representing information over long sequences.</p>

<h2 id="transformer">Transformer</h2>

<p>The <a href="http://arxiv.org/abs/1706.03762">Transformer</a> model removes the need for an RNN by using only <em>self-attention</em>. Each layer of both the encoder and decoder transforms the embedding of each token of the sequence in to a new embedding which better represents the token in its context.</p>

    <p class="mt-3">
    <ul class="tags">
    
      <li><a class="tag" href="/tags/nlp">NLP</a></li>
    
      <li><a class="tag" href="/tags/machine-learning">Machine Learning</a></li>
    
</ul>

    </p>
  </div>
</section>


    <span style="color: #999999; font-size: 60%;">Nifty <a href="https://codepen.io/wbeeftink/pen/dIaDH">tech tag lists</a> fromÂ <a class="pen-owner-link" href="https://codepen.io/wbeeftink">Wouter Beeftink</a> </span>
    
  </div>
  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/4.0.0/js/bootstrap.bundle.min.js"></script>

  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery-easing/1.4.1/jquery.easing.min.js"></script>
  
  <script src="/js/resume.js"></script>
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-131805281-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-131805281-1');
  </script>
  

  
</body>
</html>
