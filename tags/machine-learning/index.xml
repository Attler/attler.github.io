<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Machine Learning on J Hepburn</title>
    <link>https://jhepburn.io/tags/machine-learning/</link>
    <description>Recent content in Machine Learning on J Hepburn</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-AU</language>
    <lastBuildDate>Fri, 19 Jul 2019 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://jhepburn.io/tags/machine-learning/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Context in nlp</title>
      <link>https://jhepburn.io/publications/context-in-nlp/</link>
      <pubDate>Fri, 19 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>https://jhepburn.io/publications/context-in-nlp/</guid>
      <description>One of the most important problems in Natural Langugage Understanding (NLU) is context. Not all words are different to each other in the same way, which is why models based on word frequency can perform poorly. Word embeddings such as GLOVE are often used in order to represent the similarities and differences words or tokens. Words that are similar in meaning are represented closer together in the embedding space. Some words though have very different meanings in different contexts.</description>
    </item>
    
  </channel>
</rss>